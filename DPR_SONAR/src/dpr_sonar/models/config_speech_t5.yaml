optimizer: adam
dropout: 0.2
transformer_enc_positional_dropout_rate: 0.1
max_text_positions: 600
activation_dropout: 0.2
attention_dropout: 0.2
encoder_layerdrop:  0.05
clip_norm: 1.0
lr: 0.0001
lr_scheduler: inverse_sqrt
warmup_updates: 6000 
feature_grad_mult: 1.0
encoder_embed_dim: 768
encoder_ffn_embed_dim: 3072
encoder_layers: 12
encoder_attention_heads: 12
encoder_speech_prenet: conv
conv_feature_layers: "[(512, 10, 5),(512, 3, 2),(512, 3, 2),(512, 3, 2),(512, 3, 2),(512, 2, 2),(512, 2, 2)]"
extractor_mode: default
conv_bias: False
quant_noise_pq: 0
label_rates: -1
sample_rate: 16000.0
final_dim: 256
decoder_layerdrop: 0.05
dropout_input: 0.1
dropout_features: 0.1
freeze_encoder_updates: 20000
layer_norm_first: False
layer_norm_eps: 1e-05
use_conv_pos: True
conv_pos: 128
conv_pos_groups: 16
mask_prob: 0.75
mask_selection: static
mask_channel_length: 64
mask_channel_prob: 0.5
mask_channel_selection: static
hubert_mask_length: 10
mask_length: 'span-poisson'