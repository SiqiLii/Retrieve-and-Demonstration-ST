# @package _group_

# model type. One of [hf_bert, pytext_bert, fairseq_roberta]
encoder_model_type: speech_t5
pretrained_model_cfg: bert-base-uncased
optimizer: adam
dropout: 0.1
transformer_enc_positional_dropout_rate: 0.1
max_text_positions: 600
enc_use_scaled_pos_enc: True
activation_dropout: 0.2
attention_dropout: 0.2
encoder_layerdrop:  0.05
clip_norm: 1.0
lr: 1e-5
lr_scheduler: inverse_sqrt
warmup_updates: 6000 
feature_grad_mult: 0.0
encoder_embed_dim: 768
encoder_ffn_embed_dim: 3072
encoder_layers: 12
encoder_attention_heads: 12
encoder_speech_prenet: conv
conv_feature_layers: "[(512, 10, 5),(512, 3, 2),(512, 3, 2),(512, 3, 2),(512, 3, 2),(512, 2, 2),(512, 2, 2)]"
extractor_mode: default
conv_bias: False
quant_noise_pq: 0
label_rates: -1
sample_rate: 16000.0
decoder_layerdrop: 0.05
dropout_input: 0.1
dropout_features: 0.1
freeze_encoder_updates: 20000
layer_norm_first: True
layer_norm_eps: 1e-05
use_conv_pos: True
mask_prob: 0.75
mask_selection: static
mask_channel_length: 64
mask_channel_prob: 0.5
mask_channel_selection: static
hubert_mask_length: 10
mask_length: 'span-poisson'
encoder_normalize_before: False
relative_position_embedding: False
no_scale_embedding: True
use_sinc_pos: False
mask_other: 0.0
share_ctc_embed: False
freeze_decoder_updates: 0
no_freeze_encoder_layer: None
softmax_easy_margin: False
conv_pos: 128
conv_pos_groups: 16
target_glu: False
logit_temp: 0.1
final_dim: 256
untie_final_proj: True
use_sent_enc_layer: True
no_mask_overlap: False
mask_min_space: 1
mask_channel_other: 0
no_mask_channel_overlap: False
mask_channel_min_space: 1
skip_masked: False
skip_nomask: False
use_codebook: False
latent_vars: 100
latent_groups: 2
latent_dim: 0
latent_temp: (2, 0.5, 0.999995)
quantizer_depth: 1
quantizer_factor: 3
codebook_prob: 0.5
encoder_max_relative_position: 160
decoder_max_relative_position: 160
activation_fn: gelu
pad_audio: True